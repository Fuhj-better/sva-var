import json
import networkx as nx
import matplotlib.pyplot as plt
from collections import defaultdict, Counter
import numpy as np
from typing import Dict, List, Set, Any, Tuple
import statistics
import time
import re

class EnhancedVariableScoringSystem:
    def __init__(self, json_file_path):
        self.json_file_path = json_file_path
        self.data = None
        self.graph = nx.DiGraph()
        self.control_graph = nx.DiGraph()
        self.variable_scores = {}
        self.control_signals_analysis = {}
        
        # æ–°å¢åˆ†æç»“æœå­˜å‚¨
        self.timing_paths_analysis = {}
        self.fanout_analysis = {}
        self.global_signals_analysis = {}
        self.fsm_analysis = {}
        self.module_interface_analysis = {}
        self.critical_paths = []
        self.high_fanout_signals = []
        self.fsm_candidates = []
        
        # æ·»åŠ ç¼“å­˜
        self.betweenness_centrality_cached = {}
        
        self.load_data()
        
    def load_data(self):
        """åŠ è½½JSONæ•°æ®"""
        with open(self.json_file_path, 'r') as f:
            self.data = json.load(f)
        print(f"Loaded {len(self.data)} variables")
    
    def build_dependency_graph(self):
        """æ„å»ºè¯¦ç»†çš„å˜é‡ä¾èµ–å›¾ï¼ŒåŒ…æ‹¬æ•°æ®ä¾èµ–å’Œæ§åˆ¶ä¾èµ–"""
        print("Building dependency graphs...")
        
        # æ„å»ºæ•°æ®ä¾èµ–å›¾
        for var_name, var_info in self.data.items():
            self.graph.add_node(var_name, **var_info)
            
            # æ·»åŠ èµ‹å€¼ä¾èµ–è¾¹
            for assignment in var_info.get('assignments', []):
                for driving_signal in assignment.get('drivingSignals', []):
                    if driving_signal in self.data:
                        edge_attrs = {
                            'type': 'assignment',
                            'logic_type': assignment.get('logicType', 'unknown'),
                            'condition_depth': assignment.get('conditionDepth', 0),
                            'assignment_type': assignment.get('type', 'direct')
                        }
                        self.graph.add_edge(driving_signal, var_name, **edge_attrs)
            
            # æ·»åŠ fanoutä¾èµ–è¾¹
            for fanout_var in var_info.get('fanOut', []):
                if fanout_var in self.data:
                    self.graph.add_edge(var_name, fanout_var, type='fanout')
        
        print(f"Data dependency graph built: {self.graph.number_of_nodes()} nodes, {self.graph.number_of_edges()} edges")
        
        # æ„å»ºæ§åˆ¶ä¾èµ–å›¾
        self.build_control_dependency_graph()
    
    def build_control_dependency_graph(self):
        """æ„å»ºæ§åˆ¶ä¾èµ–å›¾"""
        print("Building control dependency graph...")
        
        for var_name, var_info in self.data.items():
            self.control_graph.add_node(var_name)
            
            control_signals_for_var = set()
            
            for assignment in var_info.get('assignments', []):
                control_signals = self.extract_control_signals_from_path(assignment.get('path', []))
                control_signals_for_var.update(control_signals)
                
                for control_signal in control_signals:
                    if control_signal in self.data:
                        if not self.control_graph.has_edge(control_signal, var_name):
                            self.control_graph.add_edge(control_signal, var_name, weight=1)
                        else:
                            self.control_graph[control_signal][var_name]['weight'] += 1
            
            self.control_signals_analysis[var_name] = {
                'controlled_by': list(control_signals_for_var),
                'control_frequency': len(control_signals_for_var)
            }
        
        print(f"Control dependency graph built: {self.control_graph.number_of_nodes()} nodes, {self.control_graph.number_of_edges()} edges")
    
    def extract_control_signals_from_path(self, path):
        """ä»æ¡ä»¶è·¯å¾„ä¸­æå–æ§åˆ¶ä¿¡å·"""
        control_signals = set()
        
        for clause in path:
            expr_info = clause.get('expr', {})
            signals = expr_info.get('involvedSignals', [])
            control_signals.update(signals)
        
        return control_signals

    # =====================================================================
    # æ–°å¢åŠŸèƒ½ 1: æ—¶åºè·¯å¾„åˆ†æ â­â­â­â­â­
    # =====================================================================
    
    def analyze_timing_paths(self):
        """åˆ†æå…³é”®æ—¶åºè·¯å¾„ - å¯¹æ€§èƒ½è‡³å…³é‡è¦"""
        print("\n" + "="*80)
        print("TIMING PATH ANALYSIS â­â­â­â­â­")
        print("="*80)
        
        start_time = time.time()
        
        # è¯†åˆ«æ‰€æœ‰æ—¶åºå…ƒç´ ï¼ˆå¯„å­˜å™¨ï¼‰
        sequential_vars = [
            var_name for var_name, var_info in self.data.items()
            if any(a.get('logicType') == 'sequential' for a in var_info.get('assignments', []))
        ]
        
        print(f"Found {len(sequential_vars)} sequential elements")
        
        # åˆ†ææ¯ä¸ªå¯„å­˜å™¨çš„è·¯å¾„æ·±åº¦
        for var_name in sequential_vars:
            analysis = self.analyze_register_paths(var_name)
            self.timing_paths_analysis[var_name] = analysis
        
        # è¯†åˆ«å…³é”®è·¯å¾„ï¼ˆæœ€é•¿ç»„åˆé€»è¾‘è·¯å¾„ï¼‰
        self.critical_paths = self.identify_critical_paths()
        
        end_time = time.time()
        print(f"Timing path analysis completed in {end_time - start_time:.2f} seconds")
        
        return self.critical_paths
    
    def analyze_register_paths(self, reg_name):
        """åˆ†æä»å¯„å­˜å™¨å‡ºå‘çš„è·¯å¾„"""
        analysis = {
            'max_comb_depth_forward': 0,
            'max_comb_depth_backward': 0,
            'fanout_stages': 0,
            'drives_outputs': False,
            'critical_path_score': 0.0,
            'is_critical_register': False
        }
        
        # å‰å‘è·¯å¾„åˆ†æ
        forward_depth = self.calculate_combinational_depth(reg_name, direction='forward')
        analysis['max_comb_depth_forward'] = forward_depth
        
        # åå‘è·¯å¾„åˆ†æ
        backward_depth = self.calculate_combinational_depth(reg_name, direction='backward')
        analysis['max_comb_depth_backward'] = backward_depth
        
        # æ£€æŸ¥æ˜¯å¦ç›´æ¥é©±åŠ¨è¾“å‡º
        var_info = self.data.get(reg_name, {})
        analysis['drives_outputs'] = var_info.get('drivesOutput', False)
        
        # è®¡ç®—å…³é”®è·¯å¾„è¯„åˆ†
        analysis['critical_path_score'] = (forward_depth * 0.6 + backward_depth * 0.4)
        analysis['is_critical_register'] = analysis['critical_path_score'] > 5.0
        
        return analysis
    
    def calculate_combinational_depth(self, start_var, direction='forward', max_depth=20):
        """è®¡ç®—ç»„åˆé€»è¾‘æ·±åº¦ï¼ˆä¸ç»è¿‡æ—¶åºå…ƒç´ ï¼‰"""
        visited = set()
        max_comb_depth = 0
        
        def dfs(var, depth):
            nonlocal max_comb_depth
            
            if depth > max_depth or var in visited:
                return
            
            visited.add(var)
            var_info = self.data.get(var, {})
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶åºé€»è¾‘
            is_sequential = any(
                a.get('logicType') == 'sequential' 
                for a in var_info.get('assignments', [])
            )
            
            # å¦‚æœé‡åˆ°æ—¶åºé€»è¾‘ä¸”ä¸æ˜¯èµ·ç‚¹ï¼Œåœæ­¢
            if is_sequential and var != start_var:
                max_comb_depth = max(max_comb_depth, depth)
                return
            
            # ç»§ç»­éå†
            if direction == 'forward':
                neighbors = list(self.graph.successors(var))
            else:
                neighbors = list(self.graph.predecessors(var))
            
            if not neighbors:
                max_comb_depth = max(max_comb_depth, depth)
            else:
                for neighbor in neighbors:
                    dfs(neighbor, depth + 1)
        
        dfs(start_var, 0)
        return max_comb_depth
    
    def identify_critical_paths(self, top_n=15):
        """è¯†åˆ«Top-Nå…³é”®è·¯å¾„"""
        critical_paths = []
        
        for var_name, analysis in self.timing_paths_analysis.items():
            score = analysis['critical_path_score']
            if score > 0:
                critical_paths.append((var_name, score, analysis))
        
        critical_paths.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ”´ Top {top_n} Critical Timing Paths (Performance Bottlenecks):")
        print("-" * 80)
        for i, (var_name, score, analysis) in enumerate(critical_paths[:top_n]):
            critical_marker = " âš ï¸" if analysis['is_critical_register'] else ""
            print(f"{i+1:2d}. {var_name}{critical_marker}")
            print(f"    Critical Score: {score:.2f}")
            print(f"    Forward Depth: {analysis['max_comb_depth_forward']}")
            print(f"    Backward Depth: {analysis['max_comb_depth_backward']}")
            print(f"    Drives Output: {analysis['drives_outputs']}")
        
        return critical_paths[:top_n]

    # =====================================================================
    # æ–°å¢åŠŸèƒ½ 2: æ‰‡å‡ºè´Ÿè½½åˆ†æ â­â­â­â­
    # =====================================================================
    
    def analyze_fanout_load(self):
        """åˆ†ææ‰‡å‡ºè´Ÿè½½ - é«˜æ‰‡å‡ºä¿¡å·æ˜¯è®¾è®¡ç“¶é¢ˆ"""
        print("\n" + "="*80)
        print("FANOUT LOAD ANALYSIS â­â­â­â­")
        print("="*80)
        
        start_time = time.time()
        
        for var_name, var_info in self.data.items():
            fanout_list = var_info.get('fanOut', [])
            fanout_count = len(fanout_list)
            
            # åˆ†ææ‰‡å‡ºçš„å±‚æ¬¡ç»“æ„
            fanout_depth = self.calculate_fanout_depth(var_name)
            
            # åˆ†ææ‰‡å‡ºçš„ç±»å‹åˆ†å¸ƒ
            fanout_types = self.analyze_fanout_types(var_name, fanout_list)
            
            # è®¡ç®—æœ‰æ•ˆæ‰‡å‡ºï¼ˆå»é™¤ä¸­é—´èŠ‚ç‚¹ï¼‰
            effective_fanout = self.calculate_effective_fanout(var_name)
            
            self.fanout_analysis[var_name] = {
                'direct_fanout': fanout_count,
                'fanout_depth': fanout_depth,
                'effective_fanout': effective_fanout,
                'fanout_types': fanout_types,
                'fanout_pressure': self.calculate_fanout_pressure(fanout_count, fanout_depth),
                'is_high_fanout': fanout_count > 10,
                'is_critical_fanout': fanout_count > 20  # æ›´é«˜é˜ˆå€¼
            }
        
        # è¯†åˆ«é«˜æ‰‡å‡ºç“¶é¢ˆ
        self.high_fanout_signals = self.identify_high_fanout_bottlenecks()
        
        end_time = time.time()
        print(f"Fanout analysis completed in {end_time - start_time:.2f} seconds")
        
        return self.high_fanout_signals
    
    def calculate_fanout_depth(self, var_name, max_depth=5):
        """è®¡ç®—æ‰‡å‡ºçš„æœ€å¤§æ·±åº¦"""
        visited = set()
        max_depth_found = 0
        
        def dfs(var, depth):
            nonlocal max_depth_found
            
            if depth > max_depth or var in visited:
                return
            
            visited.add(var)
            max_depth_found = max(max_depth_found, depth)
            
            var_info = self.data.get(var, {})
            fanout_list = var_info.get('fanOut', [])
            
            for fanout_var in fanout_list:
                if fanout_var in self.data:
                    dfs(fanout_var, depth + 1)
        
        dfs(var_name, 0)
        return max_depth_found
    
    def analyze_fanout_types(self, var_name, fanout_list):
        """åˆ†ææ‰‡å‡ºç›®æ ‡çš„ç±»å‹åˆ†å¸ƒ"""
        types = {
            'sequential': 0,
            'combinational': 0,
            'output': 0,
            'control': 0
        }
        
        for fanout_var in fanout_list:
            if fanout_var not in self.data:
                continue
            
            var_info = self.data[fanout_var]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯è¾“å‡º
            if var_info.get('direction') == 'output':
                types['output'] += 1
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ§åˆ¶å˜é‡
            if var_info.get('isControlVariable', False):
                types['control'] += 1
            
            # æ£€æŸ¥é€»è¾‘ç±»å‹
            assignments = var_info.get('assignments', [])
            if any(a.get('logicType') == 'sequential' for a in assignments):
                types['sequential'] += 1
            else:
                types['combinational'] += 1
        
        return types
    
    def calculate_effective_fanout(self, var_name):
        """è®¡ç®—æœ‰æ•ˆæ‰‡å‡ºï¼ˆåˆ°è¾¾è¾“å‡ºæˆ–å¯„å­˜å™¨çš„è·¯å¾„æ•°ï¼‰"""
        effective_count = 0
        visited = set()
        
        def dfs(var):
            nonlocal effective_count
            
            if var in visited:
                return
            visited.add(var)
            
            var_info = self.data.get(var, {})
            if not var_info:
                return
            
            # å¦‚æœæ˜¯è¾“å‡ºæˆ–å¯„å­˜å™¨ï¼Œè®¡æ•°
            is_output = var_info.get('direction') == 'output'
            is_sequential = any(
                a.get('logicType') == 'sequential' 
                for a in var_info.get('assignments', [])
            )
            
            if is_output or (is_sequential and var != var_name):
                effective_count += 1
                return
            
            # ç»§ç»­éå†
            for fanout_var in var_info.get('fanOut', []):
                if fanout_var in self.data:
                    dfs(fanout_var)
        
        dfs(var_name)
        return effective_count
    
    def calculate_fanout_pressure(self, fanout_count, fanout_depth):
        """è®¡ç®—æ‰‡å‡ºå‹åŠ›æŒ‡æ ‡"""
        return fanout_count * (1 + fanout_depth * 0.5)
    
    def identify_high_fanout_bottlenecks(self, top_n=15):
        """è¯†åˆ«é«˜æ‰‡å‡ºç“¶é¢ˆ"""
        bottlenecks = []
        
        for var_name, analysis in self.fanout_analysis.items():
            if analysis['is_high_fanout']:
                pressure = analysis['fanout_pressure']
                bottlenecks.append((var_name, pressure, analysis))
        
        bottlenecks.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸ”´ Top {top_n} High Fanout Bottlenecks (Timing Risks):")
        print("-" * 80)
        for i, (var_name, pressure, analysis) in enumerate(bottlenecks[:top_n]):
            critical_marker = " âš¡" if analysis['is_critical_fanout'] else ""
            print(f"{i+1:2d}. {var_name}{critical_marker}")
            print(f"    Fanout Pressure: {pressure:.2f}")
            print(f"    Direct Fanout: {analysis['direct_fanout']}")
            print(f"    Effective Fanout: {analysis['effective_fanout']}")
            print(f"    Fanout Depth: {analysis['fanout_depth']}")
            print(f"    Types: Seq={analysis['fanout_types']['sequential']}, "
                  f"Comb={analysis['fanout_types']['combinational']}, "
                  f"Out={analysis['fanout_types']['output']}")
        
        return bottlenecks[:top_n]

    # =====================================================================
    # æ–°å¢åŠŸèƒ½ 3: å¤ä½/æ—¶é’Ÿå…¨å±€ä¿¡å·åˆ†æ â­â­â­â­
    # =====================================================================
    
    def analyze_global_signals(self):
        """åˆ†æå¤ä½ã€æ—¶é’Ÿç­‰å…¨å±€ä¿¡å· - éœ€è¦ç‰¹æ®Šå…³æ³¨"""
        print("\n" + "="*80)
        print("GLOBAL SIGNALS ANALYSIS (Clock/Reset) â­â­â­â­")
        print("="*80)
        
        start_time = time.time()
        
        # è¯†åˆ«æ¨¡å¼
        clock_patterns = [r'clk', r'clock', r'ck']
        reset_patterns = [r'rst', r'reset', r'rstn', r'rst_ni?']
        enable_patterns = [r'_en$', r'enable', r'_ce$', r'valid', r'ready']
        
        for var_name, var_info in self.data.items():
            var_lower = var_name.lower()
            
            signal_type = 'regular'
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—¶é’Ÿ
            if any(re.search(pattern, var_lower) for pattern in clock_patterns):
                signal_type = 'clock'
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¤ä½
            elif any(re.search(pattern, var_lower) for pattern in reset_patterns):
                signal_type = 'reset'
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä½¿èƒ½æˆ–æ¡æ‰‹ä¿¡å·
            elif any(re.search(pattern, var_lower) for pattern in enable_patterns):
                signal_type = 'enable'
            
            # åˆ†æå…¨å±€ä¿¡å·çš„å½±å“èŒƒå›´
            if signal_type != 'regular':
                analysis = self.analyze_global_signal_impact(var_name, signal_type)
                self.global_signals_analysis[var_name] = analysis
        
        # ç”ŸæˆæŠ¥å‘Š
        self.report_global_signals()
        
        end_time = time.time()
        print(f"Global signals analysis completed in {end_time - start_time:.2f} seconds")
    
    def analyze_global_signal_impact(self, signal_name, signal_type):
        """åˆ†æå…¨å±€ä¿¡å·çš„å½±å“"""
        var_info = self.data.get(signal_name, {})
        
        # ç›´æ¥å½±å“
        direct_fanout = len(var_info.get('fanOut', []))
        
        # å½±å“çš„å¯„å­˜å™¨æ•°é‡
        affected_registers = 0
        affected_outputs = 0
        control_scope = 0
        
        for fanout_var in var_info.get('fanOut', []):
            if fanout_var not in self.data:
                continue
            
            fanout_info = self.data[fanout_var]
            
            # ç»Ÿè®¡å½±å“çš„å¯„å­˜å™¨
            if any(a.get('logicType') == 'sequential' for a in fanout_info.get('assignments', [])):
                affected_registers += 1
            
            # ç»Ÿè®¡å½±å“çš„è¾“å‡º
            if fanout_info.get('direction') == 'output':
                affected_outputs += 1
        
        # æ§åˆ¶å½±å“èŒƒå›´ï¼ˆé€šè¿‡æ§åˆ¶å›¾ï¼‰
        if signal_name in self.control_graph:
            control_scope = self.control_graph.out_degree(signal_name)
        
        return {
            'signal_type': signal_type,
            'direct_fanout': direct_fanout,
            'affected_registers': affected_registers,
            'affected_outputs': affected_outputs,
            'control_scope': control_scope,
            'criticality': self.calculate_global_signal_criticality(
                signal_type, direct_fanout, affected_registers
            )
        }
    
    def calculate_global_signal_criticality(self, signal_type, fanout, affected_regs):
        """è®¡ç®—å…¨å±€ä¿¡å·çš„å…³é”®æ€§"""
        base_score = {
            'clock': 10.0,
            'reset': 9.0,
            'enable': 7.0
        }.get(signal_type, 5.0)
        
        # æ ¹æ®å½±å“èŒƒå›´è°ƒæ•´
        fanout_factor = min(fanout / 100.0, 2.0)
        reg_factor = min(affected_regs / 50.0, 1.5)
        
        return base_score * (1 + fanout_factor + reg_factor)
    
    def report_global_signals(self):
        """ç”Ÿæˆå…¨å±€ä¿¡å·æŠ¥å‘Š"""
        if not self.global_signals_analysis:
            print("No global signals identified.")
            return
        
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = defaultdict(list)
        for sig_name, analysis in self.global_signals_analysis.items():
            by_type[analysis['signal_type']].append((sig_name, analysis))
        
        for sig_type in ['clock', 'reset', 'enable']:
            signals = by_type.get(sig_type, [])
            if not signals:
                continue
            
            signals.sort(key=lambda x: x[1]['criticality'], reverse=True)
            
            print(f"\nğŸ”µ {sig_type.upper()} Signals ({len(signals)} found):")
            print("-" * 80)
            
            for sig_name, analysis in signals[:8]:  # æ˜¾ç¤ºå‰8ä¸ª
                critical_marker = " ğŸš¨" if analysis['criticality'] > 15 else ""
                print(f"  â€¢ {sig_name}{critical_marker}")
                print(f"    Criticality: {analysis['criticality']:.2f}")
                print(f"    Direct Fanout: {analysis['direct_fanout']}")
                print(f"    Affected Registers: {analysis['affected_registers']}")
                print(f"    Control Scope: {analysis['control_scope']}")

    # =====================================================================
    # æ–°å¢åŠŸèƒ½ 4: çŠ¶æ€æœºè¯†åˆ« â­â­â­
    # =====================================================================
    
    def identify_state_machines(self):
        """è¯†åˆ«çŠ¶æ€æœº - FSMå˜é‡é€šå¸¸æ˜¯æ§åˆ¶æ ¸å¿ƒ"""
        print("\n" + "="*80)
        print("STATE MACHINE IDENTIFICATION â­â­â­")
        print("="*80)
        
        start_time = time.time()
        
        # çŠ¶æ€æœºè¯†åˆ«å¯å‘å¼è§„åˆ™
        self.fsm_candidates = []
        
        for var_name, var_info in self.data.items():
            score = self.calculate_fsm_likelihood(var_name, var_info)
            
            if score > 0.5:  # é˜ˆå€¼
                fsm_analysis = self.analyze_fsm(var_name, var_info)
                self.fsm_analysis[var_name] = fsm_analysis
                self.fsm_candidates.append((var_name, score, fsm_analysis))
        
        self.fsm_candidates.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nğŸŸ¢ Identified {len(self.fsm_candidates)} potential state machines:")
        print("-" * 80)
        
        for i, (var_name, score, analysis) in enumerate(self.fsm_candidates[:10]):
            fsm_type = "Mealy" if analysis['is_mealy'] else "Moore"
            print(f"{i+1:2d}. {var_name}")
            print(f"    FSM Likelihood: {score:.2f}")
            print(f"    Type: {fsm_type}")
            print(f"    Estimated States: {analysis['estimated_states']}")
            print(f"    Control Outputs: {analysis['control_outputs']}")
            print(f"    Transition Complexity: {analysis['transition_complexity']}")
        
        end_time = time.time()
        print(f"FSM identification completed in {end_time - start_time:.2f} seconds")
        
        return self.fsm_candidates
    
    def calculate_fsm_likelihood(self, var_name, var_info):
        """è®¡ç®—å˜é‡æ˜¯çŠ¶æ€æœºçš„å¯èƒ½æ€§"""
        score = 0.0
        
        # è§„åˆ™1: åå­—åŒ…å«state/fsm
        if any(pattern in var_name.lower() for pattern in ['state', 'fsm', 'st_']):
            score += 0.4
        
        # è§„åˆ™2: æ˜¯æ—¶åºé€»è¾‘
        is_sequential = any(
            a.get('logicType') == 'sequential' 
            for a in var_info.get('assignments', [])
        )
        if is_sequential:
            score += 0.2
        
        # è§„åˆ™3: æœ‰è‡ªåé¦ˆï¼ˆçŠ¶æ€è½¬ç§»ï¼‰
        has_self_feedback = False
        for assignment in var_info.get('assignments', []):
            driving_signals = assignment.get('drivingSignals', [])
            if var_name in driving_signals:
                has_self_feedback = True
                score += 0.3
                break
        
        # è§„åˆ™4: æ§åˆ¶å¾ˆå¤šå…¶ä»–ä¿¡å·
        if var_name in self.control_graph:
            control_out = self.control_graph.out_degree(var_name)
            if control_out > 5:
                score += 0.2
        
        # è§„åˆ™5: å¤šè·¯é€‰æ‹©èµ‹å€¼
        assignments = var_info.get('assignments', [])
        if len(assignments) > 3:
            score += 0.1
        
        return min(score, 1.0)
    
    def analyze_fsm(self, var_name, var_info):
        """åˆ†æçŠ¶æ€æœºç‰¹å¾"""
        analysis = {
            'estimated_states': 0,
            'control_outputs': 0,
            'has_self_feedback': False,
            'transition_complexity': 0,
            'is_mealy': False,
            'is_moore': False
        }
        
        # ä¼°è®¡çŠ¶æ€æ•°ï¼ˆé€šè¿‡èµ‹å€¼è·¯å¾„æ•°ï¼‰
        assignments = var_info.get('assignments', [])
        analysis['estimated_states'] = len(assignments)
        
        # æ§åˆ¶è¾“å‡ºæ•°é‡
        if var_name in self.control_graph:
            analysis['control_outputs'] = self.control_graph.out_degree(var_name)
        
        # æ£€æŸ¥è‡ªåé¦ˆ
        for assignment in assignments:
            if var_name in assignment.get('drivingSignals', []):
                analysis['has_self_feedback'] = True
                break
        
        # è½¬ç§»å¤æ‚åº¦
        total_conditions = sum(
            len(a.get('path', [])) for a in assignments
        )
        analysis['transition_complexity'] = total_conditions
        
        # åˆ¤æ–­FSMç±»å‹
        input_dependency_count = 0
        for assignment in assignments:
            for ds in assignment.get('drivingSignals', []):
                ds_info = self.data.get(ds, {})
                if ds_info.get('direction') == 'input':
                    input_dependency_count += 1
        
        if input_dependency_count > len(assignments) / 2:
            analysis['is_mealy'] = True
        else:
            analysis['is_moore'] = True
        
        return analysis

    # =====================================================================
    # æ–°å¢åŠŸèƒ½ 5: è·¨æ¨¡å—ä¾èµ–åˆ†æ â­â­â­
    # =====================================================================
    
    def analyze_module_interfaces(self):
        """åˆ†æè·¨æ¨¡å—ä¾èµ– - æ¨¡å—é—´æ¥å£ä¿¡å·é‡è¦æ€§é«˜"""
        print("\n" + "="*80)
        print("MODULE INTERFACE ANALYSIS â­â­â­")
        print("="*80)
        
        start_time = time.time()
        
        # è¯†åˆ«æ‰€æœ‰æ¥å£ä¿¡å·
        interface_signals = {
            'inputs': [],
            'outputs': [],
            'inouts': []
        }
        
        for var_name, var_info in self.data.items():
            direction = var_info.get('direction', '')
            if direction in ['input', 'output', 'inout']:
                interface_signals[direction + 's'].append(var_name)
                
                # åˆ†ææ¯ä¸ªæ¥å£ä¿¡å·
                analysis = self.analyze_interface_signal(var_name, var_info, direction)
                self.module_interface_analysis[var_name] = analysis
        
        # ç”Ÿæˆæ¥å£æŠ¥å‘Š
        self.report_module_interfaces(interface_signals)
        
        end_time = time.time()
        print(f"Module interface analysis completed in {end_time - start_time:.2f} seconds")
    
    def analyze_interface_signal(self, sig_name, sig_info, direction):
        """åˆ†æå•ä¸ªæ¥å£ä¿¡å·"""
        analysis = {
            'direction': direction,
            'bit_width': sig_info.get('bitWidth', 1),
            'internal_fanout': 0,
            'internal_fanin': 0,
            'is_control_interface': False,
            'is_data_interface': False,
            'complexity_score': 0.0,
            'is_critical_interface': False
        }
        
        # å†…éƒ¨æ‰‡å‡º/æ‰‡å…¥
        if direction == 'input':
            analysis['internal_fanout'] = len(sig_info.get('fanOut', []))
        elif direction == 'output':
            analysis['internal_fanin'] = len(sig_info.get('assignments', []))
        
        # åˆ¤æ–­æ˜¯æ§åˆ¶æ¥å£è¿˜æ˜¯æ•°æ®æ¥å£
        if sig_name in self.control_graph:
            control_degree = self.control_graph.out_degree(sig_name) if direction == 'input' else self.control_graph.in_degree(sig_name)
            if control_degree > 3:
                analysis['is_control_interface'] = True
        
        if analysis['bit_width'] > 1 and not analysis['is_control_interface']:
            analysis['is_data_interface'] = True
        
        # å¤æ‚åº¦è¯„åˆ†
        if direction == 'input':
            analysis['complexity_score'] = analysis['internal_fanout'] * (1 + analysis['bit_width'] / 10.0)
        else:
            analysis['complexity_score'] = analysis['internal_fanin'] * (1 + analysis['bit_width'] / 10.0)
        
        analysis['is_critical_interface'] = analysis['complexity_score'] > 15.0
        
        return analysis
    
    def report_module_interfaces(self, interface_signals):
        """ç”Ÿæˆæ¨¡å—æ¥å£æŠ¥å‘Š"""
        print(f"\nğŸŸ£ Interface Summary:")
        print(f"  Inputs: {len(interface_signals['inputs'])}")
        print(f"  Outputs: {len(interface_signals['outputs'])}")
        print(f"  Inouts: {len(interface_signals['inouts'])}")
        
        # è¯†åˆ«å…³é”®æ¥å£
        critical_interfaces = []
        for sig_name, analysis in self.module_interface_analysis.items():
            if analysis['is_critical_interface']:
                critical_interfaces.append((sig_name, analysis))
        
        critical_interfaces.sort(key=lambda x: x[1]['complexity_score'], reverse=True)
        
        print(f"\nğŸ”´ Top Critical Interface Signals:")
        print("-" * 80)
        for i, (sig_name, analysis) in enumerate(critical_interfaces[:10]):
            interface_type = "Control" if analysis['is_control_interface'] else "Data"
            critical_marker = " ğŸ”¥" if analysis['complexity_score'] > 30 else ""
            print(f"{i+1:2d}. {sig_name}{critical_marker}")
            print(f"    Type: {interface_type} {analysis['direction']}")
            print(f"    Complexity Score: {analysis['complexity_score']:.2f}")
            print(f"    Bit Width: {analysis['bit_width']}")
            if analysis['direction'] == 'input':
                print(f"    Internal Fanout: {analysis['internal_fanout']}")
            else:
                print(f"    Internal Fanin: {analysis['internal_fanin']}")

    # =====================================================================
    # ç»¼åˆè¯„åˆ†å’ŒæŠ¥å‘Šç”Ÿæˆ
    # =====================================================================
    
    def run_comprehensive_analysis(self):
        """è¿è¡Œå®Œæ•´çš„ç»¼åˆåˆ†æ"""
        print("ğŸš€ Starting Comprehensive Variable Analysis")
        print("=" * 80)
        
        # æ„å»ºåŸºç¡€ä¾èµ–å›¾
        self.build_dependency_graph()
        
        # è¿è¡Œæ‰€æœ‰åˆ†æå¹¶éªŒè¯
        analyses = [
            ('Timing Paths', self.analyze_timing_paths),
            ('Fanout Load', self.analyze_fanout_load),
            ('Global Signals', self.analyze_global_signals),
            ('State Machines', self.identify_state_machines),
            ('Module Interfaces', self.analyze_module_interfaces)
        ]
        
        for name, analysis_func in analyses:
            print(f"\nğŸ”§ Running {name} Analysis...")
            try:
                result = analysis_func()
                print(f"âœ… {name} Analysis Completed")
            except Exception as e:
                print(f"âŒ {name} Analysis Failed: {e}")
        
        # è®¡ç®—åŸºç¡€æŒ‡æ ‡
        print(f"\nğŸ”§ Calculating Base Metrics...")
        self.calculate_all_metrics_fast()
        
        # å½’ä¸€åŒ–è¯„åˆ†
        print(f"\nğŸ”§ Normalizing Scores...")
        self.normalize_scores()
        
        # æ•´åˆå¢å¼ºåˆ†æç»“æœ
        print(f"\nğŸ”§ Integrating Enhanced Metrics...")
        self.integrate_enhanced_metrics()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self.generate_comprehensive_report()
        
        # å¯¼å‡ºå®Œæ•´ç»“æœ
        self.export_comprehensive_results('comprehensive_analysis_results.json')
        
        print("\nâœ… All analyses completed!")
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š COMPREHENSIVE ANALYSIS SUMMARY")
        print("="*80)
        
        # å…³é”®å‘ç°ç»Ÿè®¡
        print(f"\nğŸ“ˆ Key Findings:")
        print(f"  â€¢ Critical Timing Paths: {len(self.critical_paths)}")
        print(f"  â€¢ High Fanout Signals: {len(self.high_fanout_signals)}")
        print(f"  â€¢ Global Signals: {len(self.global_signals_analysis)}")
        print(f"  â€¢ State Machines: {len(self.fsm_candidates)}")
        print(f"  â€¢ Critical Interfaces: {len([v for v in self.module_interface_analysis.values() if v['is_critical_interface']])}")
        
        # è®¾è®¡è´¨é‡æŒ‡æ ‡
        print(f"\nğŸ¯ Design Quality Indicators:")
        
        # å¹³å‡ç»„åˆé€»è¾‘æ·±åº¦
        avg_comb_depth = np.mean([a['critical_path_score'] for a in self.timing_paths_analysis.values()])
        print(f"  â€¢ Average Combinational Depth: {avg_comb_depth:.2f}")
        
        # é«˜æ‰‡å‡ºä¿¡å·æ¯”ä¾‹
        high_fanout_ratio = len(self.high_fanout_signals) / len(self.data) * 100
        print(f"  â€¢ High Fanout Signals: {high_fanout_ratio:.1f}%")
        
        # æ§åˆ¶å¤æ‚åº¦
        avg_control_out = np.mean([self.control_graph.out_degree(var) for var in self.data.keys() if var in self.control_graph])
        print(f"  â€¢ Average Control Out Degree: {avg_control_out:.2f}")
        
        print(f"\nğŸ’¡ Verification Priority Recommendations:")
        print(f"  1. Focus on {len(self.critical_paths)} critical timing paths")
        print(f"  2. Address {len(self.high_fanout_signals)} high fanout bottlenecks") 
        print(f"  3. Verify {len(self.fsm_candidates)} state machine behaviors")
        print(f"  4. Test global signal integrity")
        print(f"  5. Validate critical module interfaces")

    def integrate_enhanced_metrics(self):
        """å°†å¢å¼ºåˆ†æç»“æœæ•´åˆåˆ°variable_scoresä¸­"""
        print("\nIntegrating enhanced analysis results...")
        
        for var_name in self.data.keys():
            # ç¡®ä¿variable_scoresä¸­å­˜åœ¨è¯¥å˜é‡
            if var_name not in self.variable_scores:
                self.variable_scores[var_name] = {}
            
            scores = self.variable_scores[var_name]
            
            # æ•´åˆæ—¶åºè·¯å¾„åˆ†æ
            if var_name in self.timing_paths_analysis:
                timing_analysis = self.timing_paths_analysis[var_name]
                scores.update({
                    'critical_path_score': timing_analysis['critical_path_score'],
                    'max_comb_depth_forward': timing_analysis['max_comb_depth_forward'],
                    'max_comb_depth_backward': timing_analysis['max_comb_depth_backward'],
                    'is_critical_register': timing_analysis['is_critical_register'],
                    'drives_outputs': timing_analysis['drives_outputs']
                })
            else:
                # å¯¹äºéå¯„å­˜å™¨å˜é‡ï¼Œè®¾ç½®é»˜è®¤å€¼
                scores.update({
                    'critical_path_score': 0.0,
                    'max_comb_depth_forward': 0,
                    'max_comb_depth_backward': 0,
                    'is_critical_register': False,
                    'drives_outputs': False
                })
            
            # æ•´åˆæ‰‡å‡ºè´Ÿè½½åˆ†æ
            if var_name in self.fanout_analysis:
                fanout_analysis = self.fanout_analysis[var_name]
                scores.update({
                    'fanout_pressure': fanout_analysis['fanout_pressure'],
                    'direct_fanout': fanout_analysis['direct_fanout'],
                    'effective_fanout': fanout_analysis['effective_fanout'],
                    'fanout_depth': fanout_analysis['fanout_depth'],
                    'is_high_fanout': fanout_analysis['is_high_fanout'],
                    'is_critical_fanout': fanout_analysis['is_critical_fanout']
                })
            else:
                # è®¾ç½®é»˜è®¤å€¼
                var_info = self.data.get(var_name, {})
                fanout_count = len(var_info.get('fanOut', []))
                scores.update({
                    'fanout_pressure': 0.0,
                    'direct_fanout': fanout_count,
                    'effective_fanout': 0,
                    'fanout_depth': 0,
                    'is_high_fanout': False,
                    'is_critical_fanout': False
                })
            
            # æ•´åˆå…¨å±€ä¿¡å·åˆ†æ
            if var_name in self.global_signals_analysis:
                global_analysis = self.global_signals_analysis[var_name]
                scores.update({
                    'global_signal_criticality': global_analysis['criticality'],
                    'signal_type': global_analysis['signal_type'],
                    'affected_registers': global_analysis['affected_registers'],
                    'global_direct_fanout': global_analysis['direct_fanout'],
                    'global_control_scope': global_analysis['control_scope']
                })
            else:
                # è®¾ç½®é»˜è®¤å€¼
                scores.update({
                    'global_signal_criticality': 0.0,
                    'signal_type': 'regular',
                    'affected_registers': 0,
                    'global_direct_fanout': 0,
                    'global_control_scope': 0
                })
            
            # æ•´åˆçŠ¶æ€æœºåˆ†æ
            if var_name in self.fsm_analysis:
                fsm_analysis = self.fsm_analysis[var_name]
                fsm_score = self.get_fsm_score(var_name)
                scores.update({
                    'fsm_likelihood': fsm_score,
                    'estimated_states': fsm_analysis['estimated_states'],
                    'transition_complexity': fsm_analysis['transition_complexity'],
                    'fsm_type': 'mealy' if fsm_analysis['is_mealy'] else 'moore',
                    'fsm_control_outputs': fsm_analysis['control_outputs'],
                    'has_self_feedback': fsm_analysis['has_self_feedback']
                })
            else:
                # è®¾ç½®é»˜è®¤å€¼
                scores.update({
                    'fsm_likelihood': 0.0,
                    'estimated_states': 0,
                    'transition_complexity': 0,
                    'fsm_type': 'none',
                    'fsm_control_outputs': 0,
                    'has_self_feedback': False
                })
            
            # æ•´åˆæ¥å£åˆ†æ
            if var_name in self.module_interface_analysis:
                interface_analysis = self.module_interface_analysis[var_name]
                scores.update({
                    'interface_complexity': interface_analysis['complexity_score'],
                    'is_critical_interface': interface_analysis['is_critical_interface'],
                    'interface_type': 'control' if interface_analysis['is_control_interface'] else 'data',
                    'internal_fanout': interface_analysis['internal_fanout'],
                    'internal_fanin': interface_analysis['internal_fanin']
                })
            else:
                # è®¾ç½®é»˜è®¤å€¼
                var_info = self.data.get(var_name, {})
                direction = var_info.get('direction', '')
                if direction == 'input':
                    internal_fanout = len(var_info.get('fanOut', []))
                    scores.update({
                        'interface_complexity': 0.0,
                        'is_critical_interface': False,
                        'interface_type': 'none',
                        'internal_fanout': internal_fanout,
                        'internal_fanin': 0
                    })
                elif direction == 'output':
                    internal_fanin = len(var_info.get('assignments', []))
                    scores.update({
                        'interface_complexity': 0.0,
                        'is_critical_interface': False,
                        'interface_type': 'none',
                        'internal_fanout': 0,
                        'internal_fanin': internal_fanin
                    })
                else:
                    scores.update({
                        'interface_complexity': 0.0,
                        'is_critical_interface': False,
                        'interface_type': 'none',
                        'internal_fanout': 0,
                        'internal_fanin': 0
                    })

    def get_fsm_score(self, var_name):
        """è·å–çŠ¶æ€æœºè¯„åˆ†"""
        for candidate in self.fsm_candidates:
            if candidate[0] == var_name:
                return candidate[1]  # è¿”å›FSMå¯èƒ½æ€§è¯„åˆ†
        return 0.0

    def export_comprehensive_results(self, output_file):
        """å¯¼å‡ºåŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å®Œæ•´JSON"""
        # æ•´åˆæ‰€æœ‰ç»“æœ
        comprehensive_results = {
            'metadata': {
                'total_variables': len(self.variable_scores),
                'data_graph_nodes': self.graph.number_of_nodes(),
                'data_graph_edges': self.graph.number_of_edges(),
                'control_graph_nodes': self.control_graph.number_of_nodes(),
                'control_graph_edges': self.control_graph.number_of_edges(),
                'critical_timing_paths': len(self.critical_paths),
                'high_fanout_signals': len(self.high_fanout_signals),
                'state_machines': len(self.fsm_candidates),
                'global_signals': len(self.global_signals_analysis)
            },
            'variable_scores': self.variable_scores,
            'analysis_summary': {
                'critical_paths': self.critical_paths,
                'high_fanout_signals': self.high_fanout_signals,
                'fsm_candidates': self.fsm_candidates,
                'global_signals_analysis': self.global_signals_analysis
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_results, f, indent=2, ensure_ascii=False)
        
        print(f"Comprehensive results exported to: {output_file}")

    def calculate_all_metrics_fast(self):
        """å¿«é€Ÿè®¡ç®—æ‰€æœ‰åŸºç¡€æŒ‡æ ‡"""
        print("Calculating base metrics...")
        
        total_nodes = len(self.data)
        processed = 0
        
        for var_name in self.data.keys():
            scores = {}
            
            # è®¡ç®—å¤æ‚åº¦ç»´åº¦
            scores.update(self.calculate_complexity_metrics(var_name))
            
            # è®¡ç®—ä¸­å¿ƒæ€§ç»´åº¦
            scores.update(self.calculate_centrality_metrics(var_name))
            
            # è®¡ç®—ç»“æ„ç»´åº¦
            scores.update(self.calculate_structural_metrics(var_name))
            
            # è®¡ç®—åŠŸèƒ½ç»´åº¦
            scores.update(self.calculate_functional_metrics(var_name))
            
            # è®¡ç®—å¯é æ€§ç»´åº¦
            scores.update(self.calculate_reliability_metrics(var_name))
            
            self.variable_scores[var_name] = scores
            
            processed += 1
            if processed % 100 == 0:
                print(f"Processed {processed}/{total_nodes} variables...")
        
        print(f"Base metrics calculation completed for {len(self.variable_scores)} variables")

    def calculate_complexity_metrics(self, var_name):
        """è®¡ç®—å¤æ‚åº¦ç›¸å…³æŒ‡æ ‡"""
        var_info = self.data[var_name]
        assignments = var_info.get('assignments', [])
        
        metrics = {}
        
        # 1. æ¡ä»¶å¤æ‚åº¦
        condition_depths = [assign.get('conditionDepth', 0) for assign in assignments]
        metrics['max_condition_depth'] = max(condition_depths) if condition_depths else 0
        metrics['avg_condition_depth'] = statistics.mean(condition_depths) if condition_depths else 0
        
        # 2. è·¯å¾„å¤šæ ·æ€§
        unique_paths = len(set(str(assign.get('path', [])) for assign in assignments))
        metrics['path_diversity'] = unique_paths
        
        # 3. é€»è¾‘ç±»å‹å¤æ‚åº¦
        logic_types = [assign.get('logicType', 'unknown') for assign in assignments]
        logic_type_counts = Counter(logic_types)
        metrics['sequential_assignments'] = logic_type_counts.get('sequential', 0)
        metrics['combinational_assignments'] = logic_type_counts.get('combinational', 0)
        metrics['logic_type_mix'] = len(set(logic_types))
        
        # 4. èµ‹å€¼ç±»å‹å¤æ‚åº¦
        assignment_types = [assign.get('type', 'direct') for assign in assignments]
        metrics['assignment_type_diversity'] = len(set(assignment_types))
        
        return metrics

    def calculate_centrality_metrics(self, var_name):
        """è®¡ç®—ä¸­å¿ƒæ€§ç›¸å…³æŒ‡æ ‡ - ä¿®å¤ç‰ˆæœ¬"""
        metrics = {}
        
        # 1. åº¦æ•°ä¸­å¿ƒæ€§
        try:
            in_degree = self.graph.in_degree(var_name)
            out_degree = self.graph.out_degree(var_name)
            metrics['in_degree'] = in_degree
            metrics['out_degree'] = out_degree
            metrics['total_degree'] = in_degree + out_degree
        except:
            metrics['in_degree'] = 0
            metrics['out_degree'] = 0
            metrics['total_degree'] = 0
        
        # 2. ä»‹æ•°ä¸­å¿ƒæ€§ (æ”¹è¿›è®¡ç®—)
        try:
            # å¯¹äºå¤§å›¾ï¼Œä½¿ç”¨åŸºäºåº¦æ•°çš„è¿‘ä¼¼
            total_nodes = self.graph.number_of_nodes()
            if total_nodes > 1000:  # å¤§å›¾ä½¿ç”¨è¿‘ä¼¼
                metrics['betweenness_centrality'] = metrics['total_degree'] / (total_nodes - 1)
            else:
                # å°å›¾å¯ä»¥è®¡ç®—ç²¾ç¡®å€¼
                if not hasattr(self, 'betweenness_centrality_cached'):
                    print("Calculating betweenness centrality...")
                    self.betweenness_centrality_cached = nx.betweenness_centrality(self.graph, k=min(100, total_nodes))
                metrics['betweenness_centrality'] = self.betweenness_centrality_cached.get(var_name, 0.0)
        except:
            metrics['betweenness_centrality'] = 0.0
        
        # 3. æ¥è¿‘ä¸­å¿ƒæ€§ (æ”¹è¿›è®¡ç®—)
        try:
            # ä½¿ç”¨è¿é€šåˆ†é‡å†…çš„è®¡ç®—
            if self.graph.number_of_nodes() > 500:
                # å¤§å›¾ä½¿ç”¨ç®€åŒ–è®¡ç®—
                neighbors = set(self.graph.predecessors(var_name)) | set(self.graph.successors(var_name))
                metrics['closeness_centrality'] = len(neighbors) / self.graph.number_of_nodes()
            else:
                # å°å›¾è®¡ç®—ç²¾ç¡®å€¼
                connected_component = self.graph.subgraph(nx.node_connected_component(self.graph.to_undirected(), var_name))
                if len(connected_component) > 1:
                    closeness = nx.closeness_centrality(connected_component, var_name)
                    metrics['closeness_centrality'] = closeness
                else:
                    metrics['closeness_centrality'] = 0.0
        except:
            metrics['closeness_centrality'] = 0.0
        
        return metrics

    def calculate_structural_metrics(self, var_name):
        """è®¡ç®—ç»“æ„é‡è¦æ€§æŒ‡æ ‡ - ä¿®å¤ç‰ˆæœ¬"""
        metrics = {}
        
        # 1. æ§åˆ¶å½±å“èŒƒå›´
        try:
            if var_name in self.control_graph:
                control_descendants = len(nx.descendants(self.control_graph, var_name))
                metrics['control_scope'] = control_descendants
            else:
                metrics['control_scope'] = 0
        except:
            metrics['control_scope'] = 0
        
        # 2. æœ€å¤§å½±å“æ·±åº¦ (æ”¹è¿›è®¡ç®—)
        try:
            # ä½¿ç”¨å›¾éå†è®¡ç®—å®é™…æ·±åº¦
            max_depth = 0
            visited = set()
            
            def dfs(current, depth):
                nonlocal max_depth
                if depth > 10 or current in visited:  # é™åˆ¶æ·±åº¦é¿å…æ— é™å¾ªç¯
                    return
                visited.add(current)
                max_depth = max(max_depth, depth)
                
                # éå†åç»§èŠ‚ç‚¹
                for successor in self.graph.successors(current):
                    if successor != var_name:  # é¿å…è‡ªç¯
                        dfs(successor, depth + 1)
            
            dfs(var_name, 0)
            metrics['max_influence_depth'] = max_depth
        except:
            metrics['max_influence_depth'] = 0
        
        # 3. ä¾èµ–å¹¿åº¦ (ä¿®å¤è®¡ç®—)
        try:
            var_info = self.data.get(var_name, {})
            all_driving_signals = set()
            
            # ç»Ÿè®¡æ‰€æœ‰èµ‹å€¼ä¸­çš„é©±åŠ¨ä¿¡å·
            for assignment in var_info.get('assignments', []):
                driving_signals = assignment.get('drivingSignals', [])
                all_driving_signals.update(driving_signals)
                
                # åŒæ—¶ç»Ÿè®¡æ¡ä»¶è·¯å¾„ä¸­çš„ä¿¡å·
                for clause in assignment.get('path', []):
                    expr_info = clause.get('expr', {})
                    condition_signals = expr_info.get('involvedSignals', [])
                    all_driving_signals.update(condition_signals)
            
            metrics['dependency_breadth'] = len(all_driving_signals)
        except:
            metrics['dependency_breadth'] = 0
        
        return metrics

    def calculate_functional_metrics(self, var_name):
        """è®¡ç®—åŠŸèƒ½æ€§æŒ‡æ ‡"""
        var_info = self.data[var_name]
        metrics = {}
        
        # 1. é©±åŠ¨è¾“å‡ºé‡è¦æ€§
        metrics['drives_output'] = 1 if var_info.get('drivesOutput', False) else 0
        
        # 2. æ§åˆ¶å˜é‡é‡è¦æ€§
        metrics['is_control_variable'] = 1 if var_info.get('isControlVariable', False) else 0
        
        # 3. èµ‹å€¼é¢‘ç‡
        metrics['assignment_frequency'] = var_info.get('assignmentCount', 0)
        
        # 4. ç«¯å£é‡è¦æ€§
        direction = var_info.get('direction', '')
        metrics['is_input'] = 1 if direction == 'input' else 0
        metrics['is_output'] = 1 if direction == 'output' else 0
        metrics['is_inout'] = 1 if direction == 'inout' else 0
        
        # 5. æ•°æ®ä½å®½
        metrics['bit_width'] = var_info.get('bitWidth', 1)
        
        return metrics

    def calculate_reliability_metrics(self, var_name):
        """è®¡ç®—å¯é æ€§ç›¸å…³æŒ‡æ ‡"""
        metrics = {}
        var_info = self.data[var_name]
        
        # 1. æ¡ä»¶ç¨³å®šæ€§ (æ¡ä»¶ä¸­å‚æ•° vs ä¿¡å·çš„æ¯”ä¾‹)
        total_conditions = 0
        stable_conditions = 0
        
        for assignment in var_info.get('assignments', []):
            for clause in assignment.get('path', []):
                expr_info = clause.get('expr', {})
                params = set(expr_info.get('involvedParameters', []))
                signals = set(expr_info.get('involvedSignals', []))
                
                total_conditions += 1
                if len(params) > len(signals):
                    stable_conditions += 1
        
        metrics['condition_stability'] = stable_conditions / total_conditions if total_conditions > 0 else 1.0
        
        # 2. å¤ä½æ•æ„Ÿæ€§
        assignments = var_info.get('assignments', [])
        reset_related = 0
        for assignment in assignments:
            driving_signals = assignment.get('drivingSignals', [])
            if any('reset' in signal.lower() or 'rst' in signal.lower() for signal in driving_signals):
                reset_related += 1
        
        metrics['reset_sensitivity'] = reset_related / len(assignments) if assignments else 0.0
        
        return metrics

    def normalize_scores(self):
        """å¯¹è¯„åˆ†è¿›è¡Œå½’ä¸€åŒ–å¤„ç†"""
        print("Normalizing scores...")
        
        if not self.variable_scores:
            print("No scores to normalize.")
            return
        
        # æ”¶é›†æ‰€æœ‰æŒ‡æ ‡çš„æ•°å€¼
        all_metrics = defaultdict(list)
        for scores in self.variable_scores.values():
            for metric, value in scores.items():
                # è·³è¿‡å¸ƒå°”å€¼å’Œå­—ç¬¦ä¸²ç±»å‹çš„æŒ‡æ ‡
                if isinstance(value, (bool, str)):
                    continue
                all_metrics[metric].append(value)
        
        # å¯¹æ¯ä¸ªæ•°å€¼å‹æŒ‡æ ‡è¿›è¡Œmin-maxå½’ä¸€åŒ–
        normalized_scores = {}
        for var_name, scores in self.variable_scores.items():
            normalized = {}
            for metric, value in scores.items():
                # ä¿æŒå¸ƒå°”å€¼å’Œå­—ç¬¦ä¸²ç±»å‹ä¸å˜
                if isinstance(value, (bool, str)):
                    normalized[metric] = value
                    continue
                    
                if metric in all_metrics and all_metrics[metric]:
                    values = all_metrics[metric]
                    min_val = min(values)
                    max_val = max(values)
                    
                    if max_val > min_val:
                        normalized[metric] = (value - min_val) / (max_val - min_val)
                    else:
                        normalized[metric] = 0.5  # æ‰€æœ‰å€¼ç›¸ç­‰æ—¶å–ä¸­å€¼
                else:
                    normalized[metric] = value
            
            # ä¿å­˜åŸå§‹å€¼å’Œå½’ä¸€åŒ–å€¼
            normalized_scores[var_name] = {
                **normalized,  # å½’ä¸€åŒ–å€¼
                '_raw_scores': scores  # åŸå§‹å€¼
            }
        
        self.variable_scores = normalized_scores
        print("Score normalization completed")
# ä½¿ç”¨ç¤ºä¾‹
def main():
    # åˆå§‹åŒ–å¢å¼ºç‰ˆè¯„åˆ†ç³»ç»Ÿ
    scorer = EnhancedVariableScoringSystem('/data/fhj/sva-var/results/ibex_core.json')
    
    # è¿è¡Œå®Œæ•´åˆ†æ
    scorer.run_comprehensive_analysis()

if __name__ == "__main__":
    main()